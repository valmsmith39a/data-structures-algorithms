1. Foundational Knowledge
   1. client server model
   2. network protocol
2. Key characteristics of systems
   1. Things want systems to have/trading off
      1. availability
      2. latency
      3. throughput
      4. redundancy
      5. consistency
3. Components of a system
   1. Load balancer
   2. Proxies
   3. Caches
   4. Rate limiting
   5. Leader election
4. Actual tech
   1. Real existing products/services as components in a system or achieve certain characteristics of a system


REVIEW
* When type Google.com
   * Browser makes a DNS query to get the IP address of Google.com
      * IP address is unique identifier of the server (machine) holding assets of Google.com
   * Browser then makes an HTTP GET request to get the html, css, javascript bundle from Google’s servers.  
* Database: write to memory (faster but does not persist if server goes down) or disk (persists on hard drive)
* Latency: How quickly data travels from one point in system to another point in system (Ex client to server)
* Throughput: How much work a machine can perform per unit time (Ex How many requests (Gigabits of data) a server can handle per second Gbps)


***


* Client Server Architecture
   * What happens when you go to Google.com?
      * client: machine that sends data to a server
      * server: machine that listens for clients to send data to server and then sends data back to the client 
      * Browser (client), Google (server)
      * Browser doesn’t know what server represents, only knows it can request information from the server
      * DNS query (domain name system) to find out what the IP address of what Google.com is
         * DNS query: request that goes to set of servers and asks what is ip address of Google.com DNS is “phone book” of internet.
            * DNS translates domain names to IP addresses so browsers can load Internet resources
            * IP address: unique identifier for a machine
            * Can send packets of information in the form of bytes to IP addresses
            * CloudProvider (Google Cloud Platform)
               * Get a server with an  IP address from Google Cloud Platform
               * DNS queries: dig google.com (get the IP address of Google)
      * After DNS request, Chrome knows IP address
      * Chrome sends HTTP request to the IP address
         * HTTP: way to send data that server understands
         * Browser sends bytes (characters), packed into packets in a format and sent over to Google servers.
         * Source address of the request 
            * Request will contain the IP address of computer: source address of the request
            * When server gets the request, will know the IP address to send to 
      * Ports
         * Servers listens for requests on specific ports
         * 16000 ports that programs on the machine can listen to
         * A client, need to specify the port want to communicate on
      * Most clients know the port to use depending on protocol
         * HTTP protocol, will always use port 80
         * HTTPS protocol, will use port 443
         * net cat
            * terminal 1: nc -l 8081
            * terminal 2: 127.0.0.1 8081
               * type in terminal 2, will see in terminal 1
      * When type Google.com
         * Browser makes a DNS query to get the IP address of Google.com
            * IP address is unique identifier of google.com’s machine
         * Browser then makes an HTTP GET request to get the html, css, javascript bundle from Google’s servers.  
 


* Network Protocols
   * Protocol: agreed upon set of rules of interaction between 2 parties
      * Interaction between 2 machines following network protocols
      * Types of messages
      * Format of the messages
      * Response to a message/when messages sent etc.
   * IP: Internet Protocol
      * IP Address
      * Internet runs on IP, follows the Internet Protocol
         * When machine interacts with another machine, the data sent in the form of IP packet - fundamental unit of data sent from one machine to another machine.
         * IP packet made up of bytes
         * IP Packet: 2 main sections:
            * 2^16 bytes: 65,000 bytes
            * IP Header
               * Source IP address (machine packet is coming from)
               * Destination IP address (machine packet going to)
               * Total size of the packet
               * Version of the Internet protocol packet
               * IPV4 (modern internet)
               * IPV6 (used more and more)
               * 20-60 bytes
            * Data part of IP packet
            * Data fit into multiple IP packets
         * If only use Internet Protocol, don’t have a way to all packets will be sent and don’t guarantee correct order 
         * TCP comes into play: Transmission Control Protocol
            * Built on top of IP
            * Sent IP packets in an ordered, reliable, error free. 
               * If packets dropped, will know. If packets corrupted, will know.
            * In data portion of IP packet have TCP Header
               * Information about ordering of packets
               * Core idea behind TCP
                  * When machine communicate with another using TCP
                  * First create TCP connection with destination machine
                  * “Handshake” - TCP interaction where one computer says I want to connect (by sending a few packets) to the other machine 
                  * Connection established, opens connection
                  * If one of the 2 machines, doesn’t send data, times out
                  * Or if one machine wants to end the connection
               * TCP:
                  * Wrapper around IP
         * HTTP: Hypertext Transfer Protocol
            * Built on top of TCP
            * Higher level abstraction over TCP/IP
            * Request/Response paradigm
            * Machines communicate with each other using request/response
            * Can think of Requests/Response as objects with properties
            * Purpose of the request
            * HTTP Methods: GET, POST, PUT, DELETE
            * Path: /payments
* Storage
   * Database: Store/Retrieve data
      * Read/Write data
      * Set/Get data
      * Record/Query data
   * Database is a server
      * Take compute and make it the primary database of next system to design
   * Persistence
      * Persistence of data stored in database
      * Data stored in database - persist through outages
   * Disk and Memory
      * Database writes data to disk - will persist if database goes down
      * Database writes data in memory 
         * server code - array/hash table in server code, if server goes down
            * array/hash table stored won’t have that data anymore
         * read/write data in memory is much faster than write data through disk
      * If database goes down
         * Entire system goes down if database goes down
      * Distributed Storage
         * Storing data on a single machine vs storing data on multiple machines 
         * How to store data in multiple machines? 
         * Split data up?
         * Consistency issues
         * Staleness/up to date ness of data
         * up to date version of the data


* Latency and Throughput
   * 2 most important measures of a system
   * Latency: 
      * how long it takes for data to traverse a system
      * how long does it take for data to travel from one point to another point in a system* 
         * Ex. How long does it take from Client to Server, and Server to Client
         * Ex. Server: time it takes to read data from disk. Time it takes to get that data referred to latency
      * Tradeoff: higher/lower latencies
      * Examples of latencies for data transfer
         * Reading 1 MB from memory, will take 250 microseconds. 1 million microseconds in 1 second
         * 1 MB from memory takes 250 microseconds
         * 1 MB from SSD (solid state disk) takes 1000 microseconds
         * Reading data from memory a lot faster than disk
         * Send 1 MB on 1 GBPS (gigabit per second) network 
            * Will take 10,000 microseconds 
         * Read 1 MB HDD (hard disk drive) 20,000 microseconds
         * Packet from CA -> Netherlands -> CA 150,000 microseconds
         * Takes time for electricity to travel
      * Want to optimize systems for latency
         * Ex Video games, want low latency
   * Throughput:
      * How much work a machine can perform in a period of time
      * How much data can be transferred from one point to another point in a system in the given amount of time
      * Ex Gigabit Per Second (GBPS) How much work per unit time
      * Example 5 clients and all making requests to server 
         * Throughput: how many of these requests can the server handle per second (how many bits can server let through per second)
      * How to increase throughput?
         * Ex pay cloud provider to increase throughput, maybe increase the number of servers
   * Latency/Throughout may not be correlated
      * May low latency, but low throughput


* Availability
   * How resistant is a system to failure
      * Server/Database fails? 
      * Does system completely go down? 
   * How fault tolerant is your system? 
      * Can it continue operating even if parts of it fails?
   * Percentage of time where operational enough so that primary functions are satisfied?
   * Most systems have an implied guarantee to be available
   * Measure availability: % of system's uptime in a given year
      * Percentage of a year that the system is up
      * dd
   * Most systems aim for high availability, in "nines"
   * Ex 99% availability - 2 'nines' of availability (down 3.65 days per year)
      * Ex 99.9% availability - 3 'nines' of availability
      * "five nines" is the gold standardHave an explicit guarantee
      * Service Level Agreement: agreement between service provider/customers
         * agreement on availability
   * Service Level Objective: components of an SLA
      * High availabilty may come with tradeoffs maybe higher latency/lower throughput
   * Need to see which parts of system requires high availability
      * Which parts of system okay to fail
   * How to improve availability of system 
      * No single points of failure
      * If this part fails, entire system fails
   * Redundancy to eliminate single points of failure
      * Passive Redundancy
      * Multiple servers used, but if some fail, system can still function
   * Active Redundancy
      * Multiple machines that work together so that only one or others are functioning and if one fails, others will know and take over
* Caching
   * Use caching in algorithms - improve time complexity of algorithms
   * Speed up a system - improve latency of a system
   * Network requests
      * Don’t have to do those network requests can do different data transfers 
   * Store data in different location than origin that is faster to get 
   * Can have client caching
   * Can cache at server level, so don’t need to go to database
   * Can have a cache in between server/data level
   * Caching at hardware level (CPU caches, make it faster to retrieve from memory)
   * If a lot of network requests, caching would be useful
      * Cache result of network request
   * If computationally long operation
      * Request from client to server, server performs operation, returns to client level. Want to cache at server level after 1 computation so that client can get the same data back 
   * If multiple server hitting 1 database 
      * It’s not each network request, but it’s a large number of requests 
      * Use caching so don’t have to make so many database requests 
   * Loading icon the first time go to a site
      * Go to same question list, questions list cached on client
      * Instead of having user make network request every time, can cache
      * When run code on platform like playground, takes 1 second
         * Don’t have to do 1 second computation. 
         * When run using their solutions, cache solutions
   * Users read/write posts
      * Browser, server, make a request to server 
      * Posts stored in database
      * Cache those posts at server level 
      * Posts stored in database and server 
      * User makes a new post
         * Write through cache: 
            * Edit to piece of data, system writes data to cache/database at the same time 
            * Downside, have to go into database
         * Write back cache:
            * Server updates the cache
            * System will asynchronously update database with cache
            * Every 5 seconds etc.
   * Don’t want Caches with Stale data
      * So maybe don’t want to cache at server level 
         * BC Example YouTube Comments
            * Clients responding to comments cached in different servers. Possible for client to respond to old comments or comments that don’t exist on a particular server. 
            * Want to cache *in between* server and database, so all the servers have access to same cache
         * But if Like counts - that’s ok - can be caching at server level because doesn’t matter if like counts are not the most up to date.
   * Caching: 
      * good for static data
      * don’t have to worry about consistency
   * Eviction policies (how to decide how to evict data from cache)
      * Don’t have infinite space
         * Sometimes will be left with stale data
         * Want to get rid of data in caches
      * LRU policy: get rid of least recently used data 
      * LFU: least frequently used data gets rid of
      * LIFO/FIFO or random